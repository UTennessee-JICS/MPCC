\documentclass{bioinfo}
\usepackage{algorithmic}
\usepackage{algorithm}

\begin{document}
\title[Supplemental file 1]{Supplemental file to the MPCC paper}

\section{Pearson$'$s Correlation Coefficient}
When we say Pearson$'$s correlation coefficient, we mean:

\begin{equation}
%r=\frac{n\sum x_iy_i-\sum x_i\sum y_i}{((n\sum x_i^2-(\sum x_i)^2)(n\sum y_i^2-(\sum y_i)^2)^(1/2)}
r=\frac{n\sum x_iy_i-\sum x_i\sum y_i}{ \sqrt[2]{(n\sum x_i^2-(\sum x_i)^2)(n\sum x_i^2-(\sum y_i)^2)}}
\end{equation}
where $x$ and $y$ are vectors of length $n$, and $r$ is a real number between -1 and 1 inclusive.

\section{The Naive Version}

Given $A$, an arbitrary number of vectors of length $n$, and $B$, an arbitrary number of vectors of length $n$, 
we define the naive version like so:

\begin{algorithmic}[1]
\FOR{Each vector  $i$ in set of vectors $A$}
  \FOR{Each vector  $j$ in set of vectors $B$}
    \STATE $m=n$
    \STATE $SA_{ij}=SAA_{ij}=SB_{ij}=SBB_{ij}=SAB_{ij}=0$
    \FOR{Each element $k$  in vectors $i$ and $j$}
      \IF{element $k$ in vectors $i$ or $j$ missing}
        \STATE Decrement vector length: $m=m-1$
      \ELSE
        \STATE $SA_{ij}+=A_{ik}$
        \STATE $SB_{ij}+=B_{jk}$
        \STATE $SAA_{ij}+=A_{ik}A_{ik}$
        \STATE $SBB_{ij}+=B_{jk}B_{jk}$
        \STATE $SAB_{ij}+=A_{ik}B_{jk}$
      \ENDIF
    \ENDFOR 
    \STATE $R_{ij}=\frac{mSAB_{ij}-SA_{ij}SB_{ij}}{ \sqrt[2]{(mSAA_{ij}-(SA_{ij}SA_{ij}))(mSBB_{ij}-(SB_{ij}SB_{ij}))}}$
  \ENDFOR
\ENDFOR
\end{algorithmic}

\noindent where,\\ 
$SA_{ij}$ is the sums of the elements of vector $i$ in $A$.\\
$SAA_{ij}$ is the sums of the elements squared of vector $i$ in $A$.\\
$SB_{ij}$ is the sums of the elements of vector $j$ in $B$.\\
$SBB_{ij}$ is the sums of the elements squaredof vector $j$ in $B$.\\
$SAB_{ij}$ is the sums of vector $i$ in $A$ times the elements of vector $j$ in $B$.\\
Note that $R_{ij}$ is the PCC of vector $i$ of $A$ and vector $j$ of $B$.

\section{The Matrix Version}
The matrix version of PCC assembles the same Rij expression as the naive version (line 16), 
but does so in a bulk fashion, computing the various terms in large chunks independently
and then assembling the PCC expression at the end.
\begin{algorithmic}[1]

  \STATE $\sum A=\text{SGEMM(}A_Z,U_B\text{)}$
  \STATE $\sum A^2=\text{SGEMM(}A_Z^2,U_B\text{)}$
  \STATE $\sum B=\text{SGEMM(}B_Z,U_A\text{)}$
  \STATE $\sum B^2=\text{SGEMM(}B_Z^2,U_A\text{)}$
  \STATE $\sum AB=\text{SGEMM(}A_Z,B_Z\text{)}$
  
  \vspace{2mm}

  \STATE Compute $M\sum A^2$   (Element-wise multiplication)
  \STATE Compute $M\sum B^2$   (Element-wise multiplication)
  \STATE Compute $M\sum AB$   (Element-wise multiplication)
  
  \vspace{2mm}

  \STATE $N_0=M\sum AB - \sum A\sum B$   (Element-wise subtraction)
  \STATE $D_0=M\sum A^2 - \sum A\sum A$   (Element-wise subtraction)
  \STATE $D_1=M\sum B^2 - \sum B\sum B$   (Element-wise subtraction)

  \vspace{2mm}

  \STATE $D_2=D_0 D_1 $   (Element-wise multiplication)
  \STATE $D_3=\text{SQRT(}D_2\text{)}$   (Element-wise square root)

  \vspace{2mm}

  \STATE $P=N_0 / D_3$   (Element-wise division)
  
\end{algorithmic}

\noindent Where $A_Z$ is the set of vectors $A$ with a zero at every missing data location.
$U_B$ is a masking matrix of $1's$ with $0's$ in the locations in which matrix $B$ has missing data. 
$U_A$ is a masking matrix of $1's$ with $0's$ in the locations in which matrix $A$ has missing data.
$A_Z^2$ is the result of squaring every element of $A_Z$, 
$B_Z$ is the set of vectors $B$ with a zero at every missing data location, 
$B_Z^2$ is the result of squaring every element of $B_Z$, 
$M$ is a matrix whose $i, j$-th element is the number of 
elements in the $i$-th vector of $A$, and the $j$-th vector of $B$ 
such that there is no missing data in either $A$ or $B$, and $P$ is 
a matrix of PCC calculations.

\section{Missing Data Handling} \label{MDBM}
In the summation functions of the Matrix version (lines 1-4), we use masking matrices to compute 
sums efficiently. For example: $\sum A=\text{SGEMM(}A_Z,U_B\text{)}$, where $A_Z$ is the set of 
vectors $A$ with a zero at every missing data location, and $U_B$ is a matrix of $1's$ with $0's$ 
at the locations in which matrix $B$ has missing data. By using this masking matrix, we convert a 
three level iterative loop over all elements in both $A$ and $B$ matrices with a conditional check 
for every element pair in the summation into a simple and efficient matrix-matrix multiplication.

Omitted from the matrix version algorithm presented above are the preprocessing steps necessary to 
assemble missing data masks $U_A$ and $U_B$ , which are then used in the summations of lines 1-4.
Assembling the missing data masks involves a two level nested loop over both matrices $A$ and $B$ in 
which a check for missing data occurs which zeros out elements at the location of missing data for 
each of the matrices $A$, $B$, $U_A$, and $U_B$.

In addition, in the same double nested loops described for the missing data masks, to 
generate $M$ above, a separate mask for each vector in $A$ and for each vector in $B$, 
is set to zero in every position where there is missing data and set to ones in remaining positions.
Then, to find the number of positions where there is no missing data for a given vector $i$ 
in $A$ and $j$ in $B$ (i.e. a valid correlation), a pairwise product between the corresponding masks, 
and sum of the resulting vector is the number of valid
pairwise correlations for each vector comparison between $A$ and $B$. This operation when done 
on each pair of vectors $i$ and $j$ is equivalent to matrix-matrix product of masks of 
$A$ and $B$ and results in $M$.

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{img/figure01.eps}
  \caption*{ \small Genotype to genotype correlation between genetic markers on chromosome 
    1 of the BxD family. Pairwise correlation between all 7,321 marker was \textasciitilde{}6 times 
    faster using MPCC. Computation was performed on a Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz.  }
\end{figure}

\end{document}
